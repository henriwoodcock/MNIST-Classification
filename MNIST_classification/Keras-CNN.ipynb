{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks - MNIST Dataset\n",
    "\n",
    "Convolutional neural networks have been shown to be very successful in image recognition. A MLP is fully connected by each layer, for an image this can result in a huge ammount of weights to be learned, especially if the imagine is RGB you have 3x the amount of pixels. This can result in huge amounts of weights to learn.\n",
    "\n",
    "On the other hand a convolutional neural net will use a \"filter\" to scan the images. This is essentially looking over smaller portions of an image so maybe a 3x3 section of the picture. This allows for nearby pixels to be more strongly related than further away pixels, this means the model learns relationships between certain parts of images for example the shape of an eye when recognising a face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x147b36550>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "i = random.randint(1,60000)\n",
    "plt.imshow(x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train/255, x_test/255\n",
    "x_train, x_test = x_train.reshape(x_train.shape[0], 28, 28, 1), x_test.reshape(x_test.shape[0],28,28,1)\n",
    "\n",
    "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.25\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28,1)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(1-keep_prob))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(1-keep_prob))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#model.fit(x_train, y_train, \n",
    "          #batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "#score = model.evaluate(x_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 113s 2ms/sample - loss: 0.4468 - acc: 0.8593\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 117s 2ms/sample - loss: 0.2339 - acc: 0.9310\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 112s 2ms/sample - loss: 0.1955 - acc: 0.9420\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 118s 2ms/sample - loss: 0.1707 - acc: 0.9490\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 116s 2ms/sample - loss: 0.1612 - acc: 0.9523\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 134s 2ms/sample - loss: 0.1512 - acc: 0.9561\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 139s 2ms/sample - loss: 0.1455 - acc: 0.9575\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 138s 2ms/sample - loss: 0.1400 - acc: 0.9592\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 145s 2ms/sample - loss: 0.1352 - acc: 0.9598\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 147s 2ms/sample - loss: 0.1323 - acc: 0.9612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x144422cc0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size= 32 ,epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.035197954082820435, 0.988]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From just 10 epochs, the model has a 98.8% accuracy. This can even be improved with optimising the model and increasing the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
