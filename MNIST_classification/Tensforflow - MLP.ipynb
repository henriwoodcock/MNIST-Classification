{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP - Tensorflow:\n",
    "\n",
    "input layer $ \\rightarrow $ hidden layer 1 (activation function) $\\rightarrow $ hidden layer 2 (activation function) $\\rightarrow$ output layer (activation function). This is feed forward.\n",
    "\n",
    "Compare output with actual output using a loss (cost) function.\n",
    "\n",
    "Then optimsise this output by minimising loss function (altering weights and bias). This is backpropagation.\n",
    "\n",
    "feed forward + backpropagation = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "'''one hot encode y:'''\n",
    "num_of_classes = 10 #this is because there are 10 numbers in the set\n",
    "y_train = to_categorical(y_train, num_of_classes)\n",
    "y_test = to_categorical(y_test, num_of_classes)\n",
    "\n",
    "neurons_L1 = 500\n",
    "neurons_L2 = 500\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#height x width:\n",
    "#flattening x so height = none and width is equal to 28 times 28.\n",
    "x = tf.placeholder('float',[None, 784])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(data):\n",
    "    hidden_L1 = {'weights': tf.Variable(tf.random_normal([784, neurons_L1])),\n",
    "                'biases': tf.Variable(tf.random_normal([neurons_L1]))}\n",
    "    \n",
    "    hidden_L2 = {'weights': tf.Variable(tf.random_normal([neurons_L1, neurons_L2])),\n",
    "                'biases': tf.Variable(tf.random_normal([neurons_L2]))}\n",
    "\n",
    "    output_L = {'weights': tf.Variable(tf.random_normal([neurons_L2, num_of_classes])),\n",
    "                'biases': tf.Variable(tf.random_normal([num_of_classes]))}\n",
    "    \n",
    "    L1 = tf.add(tf.matmul(data, hidden_L1['weights']), hidden_L1['biases']) #matrix multiplication\n",
    "    L1 = tf.nn.relu(L1)\n",
    "    \n",
    "    L2 = tf.add(tf.matmul(L1, hidden_L2['weights']), hidden_L2['biases']) #matrix multiplication\n",
    "    L2 = tf.nn.relu(L2)\n",
    "    \n",
    "    output = tf.matmul(L2, output_L['weights']) + output_L['biases'] #matrix multiplication\n",
    "    output = tf.nn.relu(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d093eb106c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-d093eb106c0a>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_network(x)\n",
    "    loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    \n",
    "    optimiser = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    epochs = 10\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        \n",
    "        \n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "        y_train = to_categorical(y_train, num_of_classes)\n",
    "        y_test = to_categorical(y_test, num_of_classes)\n",
    "        x_train = x_train.reshape(60000, 784)\n",
    "        x_test = x_test.reshape(10000, 784)\n",
    "        \n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train = x_train/255\n",
    "        x_test = x_test/255\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        for epoch in epochs:\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for _ in range(int(x_train.shape[0] / batch_size)):\n",
    "                \n",
    "                epoch_x, epoch_y = next_batch(batch_size, x_train, y_train)\n",
    "                \n",
    "                _, l = sess.run([optimiser, loss], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                \n",
    "                epoch_loss +=l\n",
    "                \n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "                \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        return accuracy\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
