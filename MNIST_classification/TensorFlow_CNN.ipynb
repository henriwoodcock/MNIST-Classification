{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train/255, x_test/255\n",
    "x_train, x_test = x_train.reshape(x_train.shape[0], 28, 28, 1), x_test.reshape(x_test.shape[0],28,28,1)\n",
    "\n",
    "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN configuration:\n",
    "\n",
    "# Convolutional Layer 1.\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 36         # There are 36 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 128             # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float',shape = (None, 28, 28, 1), name=\"x\")\n",
    "y = tf.placeholder('float', shape = (None), name = \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features\n",
    "\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv layers:\n",
    "conv_L1 = {'weights': tf.Variable(tf.truncated_normal([filter_size1, filter_size1, 1, num_filters1], stddev=0.05)),\n",
    "            'biases': tf.Variable(tf.constant(0.05, shape=[num_filters1]))}\n",
    "\n",
    "conv_L2 = {'weights': tf.Variable(tf.truncated_normal([filter_size2, filter_size2, num_filters1, num_filters2], stddev=0.05)),\n",
    "            'biases': tf.Variable(tf.constant(0.05, shape=[num_filters2]))}\n",
    "\n",
    "\n",
    "\n",
    "L1 = tf.nn.conv2d(input = x, filter = conv_L1[\"weights\"], strides = [1,1,1,1], padding = \"SAME\",name=\"mylayer1\")\n",
    "L1 += conv_L1[\"biases\"]\n",
    "#maxpooling:\n",
    "L1 = tf.nn.max_pool(L1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = \"SAME\")\n",
    "\n",
    "#activation function:\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "\n",
    "#conv layer 2:\n",
    "L2 = tf.nn.conv2d(input = L1, filter = conv_L2[\"weights\"], strides = [1,1,1,1], padding = \"SAME\")\n",
    "L2 += conv_L2[\"biases\"]\n",
    "\n",
    "L2 = tf.nn.max_pool(L2, ksize = [1,8,8,1], strides = [1,8,8,1], padding = \"SAME\")\n",
    "\n",
    "L2 = tf.nn.relu(L2)\n",
    "\n",
    "\n",
    "#fully connected layer:\n",
    "layer_flat, num_features = flatten_layer(L2)\n",
    "\n",
    "full_con_L3 = {'weights': tf.Variable(tf.random_normal([num_features, fc_size])),\n",
    "                'biases': tf.Variable(tf.random_normal([fc_size]))}\n",
    "\n",
    "output_layer = {'weights': tf.Variable(tf.random_normal([fc_size, 10])),\n",
    "                'biases': tf.Variable(tf.random_normal([10]))}\n",
    "\n",
    "\n",
    "\n",
    "L3 = tf.add(tf.matmul(layer_flat, full_con_L3['weights']), full_con_L3['biases'])\n",
    "L3 = tf.nn.relu(L3)\n",
    "\n",
    "logits = tf.add(tf.matmul(L3, output_layer['weights']), output_layer['biases'])\n",
    "output = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y) )\n",
    "optimiser = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = 100\n",
    "\n",
    "epochs = 5\n",
    "total_batch_count = 60000//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1 loss = 0.6570533545066916 time for epoch = 44.607166051864624\n",
      "epoch =  2 loss = 0.12615318010716384 time for epoch = 42.42839193344116\n",
      "epoch =  3 loss = 0.08760699524311351 time for epoch = 41.908621072769165\n",
      "epoch =  4 loss = 0.06939670414062375 time for epoch = 42.10570788383484\n",
      "epoch =  5 loss = 0.05734044778277162 time for epoch = 42.331732988357544\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        avg_loss = 0 \n",
    "        \n",
    "        for i in range(total_batch_count):\n",
    "            \n",
    "            batch_x, batch_y = next_batch(batch_size, x_train, y_train)\n",
    "            \n",
    "            _, c = sess.run([optimiser, loss], feed_dict = {x:batch_x, y:batch_y})\n",
    "            \n",
    "            avg_loss +=c/total_batch_count\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"epoch = \", epoch + 1, \"loss =\", avg_loss, \"time for epoch =\", end - start)\n",
    "           \n",
    "        \n",
    "    preds = output.eval({x:x_test})\n",
    "    \n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set:  0.9788\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = []\n",
    "for i in range(len(preds)):\n",
    "    pred = np.argmax(preds[i])\n",
    "    acts = np.argmax([y_test[i]])\n",
    "    if pred == acts:\n",
    "        correct+=1\n",
    "    else:\n",
    "        incorrect.append(i)\n",
    "        \n",
    "print(\"accuracy on test set: \", correct/len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "model is overfitting, loss of 0.05 is very low and so test set accuracy is only 97.88%. As high as this is, it is still less than what was achieved on keras however to overcome this some regularisation can be added like dropout between the two connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
